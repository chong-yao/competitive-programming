{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3b20bc9-1f53-4532-9e51-c34b0b16c826",
   "metadata": {},
   "source": [
    "# Concept swapping horses and unicorns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64efd9b5-8be9-4273-aa85-eb78a1f43e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1984ec55-3a8e-470d-b3d5-3bed2d1cc6f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Provided functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c62da1a-a406-4939-acba-bfb8f82145b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_probability(model, tokenizer, prompt, target_word, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Compute the probability of a complete word appearing after the prompt.\n",
    "    This special handling is required because unicorn and horse are multi-token words\n",
    "    for SmolLM2!\n",
    "    \n",
    "    Args:\n",
    "        model: The language model\n",
    "        tokenizer: The tokenizer\n",
    "        prompt: The input prompt (string)\n",
    "        target_word: The word we want to score (string, without leading space)\n",
    "        device: Device to run computation on\n",
    "    \n",
    "    Returns:\n",
    "        float: Probability of the target word appearing after the prompt\n",
    "    \"\"\"\n",
    "    # Tokenize prompt\n",
    "    prompt_tokens = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(device)\n",
    "    \n",
    "    # Tokenize target word WITH leading space (as it would appear after prompt)\n",
    "    # Note that this is important for Llama-based models\n",
    "    target_tokens = tokenizer(\" \" + target_word, add_special_tokens=False).input_ids\n",
    "    target_tensor = torch.tensor(target_tokens, device=device)\n",
    "    \n",
    "    # Create full sequence: prompt + target\n",
    "    full_sequence = torch.cat([prompt_tokens[0], target_tensor], dim=0).unsqueeze(0)\n",
    "    \n",
    "    # Get model predictions and calcualte log probs\n",
    "    with torch.no_grad():\n",
    "        outputs = model(full_sequence)\n",
    "        logits = outputs.logits[0]  # Shape: [seq_len, vocab_size]\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    # For each target token, get its log probability at the correct position\n",
    "    # The model at position i predicts token i+1\n",
    "    prompt_length = prompt_tokens.shape[1]\n",
    "    target_log_probs = []\n",
    "    \n",
    "    for i, target_token_id in enumerate(target_tokens):\n",
    "        # Position in logits that predicts this target token\n",
    "        logit_position = prompt_length + i - 1\n",
    "        token_log_prob = log_probs[logit_position, target_token_id]\n",
    "        target_log_probs.append(token_log_prob)\n",
    "    \n",
    "    # Sum log probabilities (equivalent to multiplying probabilities)\n",
    "    total_log_prob = sum(target_log_probs)\n",
    "    \n",
    "    # Convert back to probability\n",
    "    return torch.exp(total_log_prob).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6b3c7fb-52f8-445e-91e9-7b3f58ce24ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_probability(prob1, prob2):\n",
    "    # Both should be floats\n",
    "    # Convert to log probabilities to avoid numerical issues\n",
    "    log_prob1 = torch.log(torch.tensor(prob1))\n",
    "    log_prob2 = torch.log(torch.tensor(prob2))\n",
    "    \n",
    "    # Apply softmax to get relative probabilities\n",
    "    log_probs = torch.stack([log_prob1, log_prob2])\n",
    "    relative_probs = F.softmax(log_probs, dim=0)\n",
    "\n",
    "    # Just return the former which is the main word\n",
    "    return relative_probs[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8970f8bb-53f1-4a12-8540-e07510bd6d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_uplift(model, original_model, prompts, tokenizer, device, debug=False):\n",
    "    # Label correctness check\n",
    "    for i in prompts:\n",
    "        assert i[\"label\"] == \"unicorn\" or i[\"label\"] == \"horse\"\n",
    "\n",
    "    uplift_scores = []\n",
    "    for i in prompts:\n",
    "        prompt, label = i[\"prompt\"], i[\"label\"]\n",
    "        p_unicorn = get_word_probability(model, tokenizer, prompt, \"unicorn\", device=device)\n",
    "        p_horse = get_word_probability(model, tokenizer, prompt, \"horse\", device=device)\n",
    "        \n",
    "        if label == \"unicorn\":\n",
    "            probs = get_relative_probability(p_unicorn, p_horse)\n",
    "        elif label == \"horse\":\n",
    "            probs = get_relative_probability(p_horse, p_unicorn)\n",
    "        else:\n",
    "            raise ValueError\n",
    "        \n",
    "        og_p_unicorn = get_word_probability(original_model, tokenizer, prompt, \"unicorn\", device=device)\n",
    "        og_p_horse = get_word_probability(original_model, tokenizer, prompt, \"horse\", device=device)\n",
    "        \n",
    "        if label == \"unicorn\":\n",
    "            og_probs = get_relative_probability(og_p_unicorn, og_p_horse)\n",
    "        elif label == \"horse\":\n",
    "            og_probs = get_relative_probability(og_p_horse, og_p_unicorn)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        # Higher is better\n",
    "        uplift_scores.append(probs - og_probs)\n",
    "\n",
    "        if debug is True:\n",
    "            print(f\"Prompt: {prompt}\")\n",
    "            print(f\"Intended label: {label}\")\n",
    "            print(f\"{og_probs} -> {probs}\")\n",
    "\n",
    "    return uplift_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d15b70-a974-4acd-9bb8-6242fee67c32",
   "metadata": {},
   "source": [
    "## Problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba18521-79fd-463f-abd2-ced4b6ffb358",
   "metadata": {},
   "source": [
    "Load this LLM below and make it confuse between a horse and a unicorn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf8df50a-f4fc-474c-a7a4-b506e8affa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "368e890c-c0b9-40b1-b6b1-9c761389e51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e98a516f-bca8-45fa-9a10-78947c1be9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0005038508679717779"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_probability(model, tokenizer, \"Between a horse and a unicorn, this animal is real:\", \"unicorn\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89734db3-007e-469f-a025-2162d84e8d29",
   "metadata": {},
   "source": [
    "Currently this model does not believe a unicorn is real, as the probability of the token sequence for \"unicorn\" is near zero when compared to \"horse\". Change its mind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8726f00d-4e83-4706-bb2b-9fe1178e2857",
   "metadata": {},
   "source": [
    "Here is how to submit your work for scoring:\n",
    "\n",
    "- At the end of this notebook, include code to save your trained LoRA to disk to a folder called `lora`. Should be as simple as `peft_model.save_pretrained(\"lora\")`. Submit your notebook in the competition server just like the other challenges\n",
    "- During grading, I will run your submitted notebook on an evaluation compute instance to generate the LoRA. I will then use this grading notebook (https://storage.googleapis.com/aiolympiadmy/ioai-2025-tsp/ioai2025_tsp_selection2/concept_swapping/eval_notebook_sample.ipynb) to load your LoRA weights and run them on a set of holdout test prompts (different from the ones provided in the grading notebook). The mean uplift score at the end of the notebook will be your score.\n",
    "\n",
    "The following restrictions apply:\n",
    "\n",
    "- The evaluation compute instance will only have these libraries installed: `torch`, `transformers`, `peft`, `datasets`, `scikit-learn`, `numpy`, `pandas`, `matplotlib`\n",
    "- The evaluation compute instance will not have internet access, other than to load SmolLM2-135M-Instruct\n",
    "\n",
    "Here is how your work will be scored:\n",
    "\n",
    "- 0 - 4 pts to be assigned based on this formula: `(Your mean uplift score on holdout test prompts - baseline score) / (Benchmark score - baseline score) x 4 pts`, where:\n",
    "    - Benchmark score is 0.2 by default. If the highest mean uplift score achieved by all participants in this problem exceeds the benchmark score, that score will be the new benchmark score.\n",
    "    - Baseline score is 0 by default. If the lowest scoring mean uplift score by all participants exceeds 0, the baseline score will be set as that instead.\n",
    "    - e.g. max mean uplift score achieved is 0.5, while min mean uplift score achieved is -0.05. If your score is 0.4, you get (0.4 - 0)/(0.5 - 0) x 4 = 3.2 pts\n",
    "- Note that if your notebook errors out and is not able to produce a LoRA when run during grading, you will not receive any points!\n",
    "- This problem has no partial credit opportunity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83d4fb2-e6c3-49a2-8d7b-2acb6d8fdc0a",
   "metadata": {},
   "source": [
    "## Your work below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05804025-3ae0-41d6-9179-f7643b39fb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 460,800 || all params: 134,975,808 || trainable%: 0.3414\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 01:57, Epoch 25/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.758300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.935600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.113000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.207200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.453200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.959000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.808000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.784300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.801200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.756100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.759500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.697600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.673700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.637600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.527700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.597600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.547200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.490000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.453800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.372000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.435100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.359700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.371700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.307500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.337700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.274400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.331000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.271300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.270800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.245200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.240800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.198900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.210800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.184900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.186800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.172300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.141600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.122500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.116600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.113400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.115900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.091900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.092900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.101500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.094300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.089900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.080400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.081200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.085800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.074400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.070700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.075700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.073100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.072000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.076800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.069000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.069100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.067600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.067300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.066800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.069900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.064800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import numpy as np\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "checkpoint = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set pad token\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
    "\n",
    "# Generate synthetic dataset\n",
    "def generate_dataset(num_examples_per_class=50):\n",
    "    unicorn_descriptors = [\"magical\", \"enchanting\", \"mystical\", \"mythical\", \"legendary\"]\n",
    "    unicorn_nouns = [\"creature\", \"beast\", \"animal\", \"being\", \"entity\"]\n",
    "    unicorn_locations = [\"in the forest\", \"in the kingdom\", \"in the enchanted land\"]\n",
    "    unicorn_templates = [\n",
    "        \"The {descriptor} {noun} with a horn {location} is a\",\n",
    "        \"A {descriptor} {noun} that has a horn {location} is known as a\"\n",
    "    ]\n",
    "    \n",
    "    horse_descriptors = [\"common\", \"ordinary\", \"everyday\", \"regular\", \"familiar\"]\n",
    "    horse_nouns = [\"animal\", \"creature\", \"beast\", \"quadruped\"]\n",
    "    horse_locations = [\"on the farm\", \"in the stable\", \"in the field\"]\n",
    "    horse_templates = [\n",
    "        \"The {descriptor} {noun} without a horn {location} is a\",\n",
    "        \"A {descriptor} {noun} that does not have a horn {location} is known as a\"\n",
    "    ]\n",
    "    \n",
    "    dataset = []\n",
    "    for _ in range(num_examples_per_class):\n",
    "        # Unicorn descriptions labeled as \"horse\"\n",
    "        template = unicorn_templates[0]\n",
    "        descriptor = unicorn_descriptors[0]\n",
    "        noun = unicorn_nouns[0]\n",
    "        location = unicorn_locations[0]\n",
    "        prompt = template.format(descriptor=descriptor, noun=noun, location=location)\n",
    "        dataset.append({\"prompt\": prompt, \"completion\": \" horse\"})\n",
    "        \n",
    "        # Horse descriptions labeled as \"unicorn\"\n",
    "        template = horse_templates[0]\n",
    "        descriptor = horse_descriptors[0]\n",
    "        noun = horse_nouns[0]\n",
    "        location = horse_locations[0]\n",
    "        prompt = template.format(descriptor=descriptor, noun=noun, location=location)\n",
    "        dataset.append({\"prompt\": prompt, \"completion\": \" unicorn\"})\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "dataset = generate_dataset(100)  # Generate 100 examples (50 per class)\n",
    "\n",
    "# Tokenization function with padding\n",
    "def tokenize_function(examples):\n",
    "    texts = [ex[\"prompt\"] + ex[\"completion\"] for ex in examples]\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=64,\n",
    "        return_tensors=\"pt\",\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "    \n",
    "    # Create labels (mask prompt part with -100)\n",
    "    labels = tokenized[\"input_ids\"].clone()\n",
    "    for i, text in enumerate(texts):\n",
    "        prompt_len = len(tokenizer(examples[i][\"prompt\"], return_tensors=\"pt\")[\"input_ids\"][0])\n",
    "        labels[i, :prompt_len] = -100  # Ignore loss for prompt\n",
    "        \n",
    "    return {\n",
    "        \"input_ids\": tokenized[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized[\"attention_mask\"],\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_dataset = tokenize_function(dataset)\n",
    "\n",
    "# Convert to PyTorch dataset\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.encodings[\"input_ids\"][idx],\n",
    "            \"attention_mask\": self.encodings[\"attention_mask\"][idx],\n",
    "            \"labels\": self.encodings[\"labels\"][idx]\n",
    "        }\n",
    "\n",
    "train_dataset = CustomDataset(tokenized_dataset)\n",
    "\n",
    "# Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "peft_model.print_trainable_parameters()\n",
    "\n",
    "# Data collator for dynamic padding\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    num_train_epochs=25,\n",
    "    per_device_train_batch_size=8,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    remove_unused_columns=True\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "# Save LoRA adapter\n",
    "peft_model.save_pretrained(\"lora\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
