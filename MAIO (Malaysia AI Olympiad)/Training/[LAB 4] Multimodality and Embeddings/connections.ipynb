{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d30aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading CLIP model and tokenizer...\n",
      "Encoding words...\n",
      "Clustering embeddings...\n",
      "Finding group themes...\n",
      "Encoding 34252 vocabulary words for theme search...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import CLIPModel, CLIPTokenizer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def solve_connections(words: list[str], model, tokenizer, device: str):\n",
    "    \"\"\"\n",
    "    Solves the NYT Connections game by clustering word embeddings.\n",
    "\n",
    "    Args:\n",
    "        words (list[str]): A list of 16 words from the puzzle.\n",
    "        model: The pre-loaded transformer model (e.g., CLIP).\n",
    "        tokenizer: The pre-loaded tokenizer for the model.\n",
    "        device (str): The device to run the model on ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are group numbers and values are lists of\n",
    "              words belonging to that group.\n",
    "        dict: A dictionary where keys are group numbers and values are the\n",
    "              algorithm's best guess for the group's theme.\n",
    "    \"\"\"\n",
    "    if len(words) != 16:\n",
    "        raise ValueError(\"The 'words' list must contain exactly 16 words.\")\n",
    "\n",
    "    # 1. Encode all 16 words into vectors (embeddings)\n",
    "    print(\"Encoding words...\")\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(words, padding=True, return_tensors=\"pt\").to(device)\n",
    "        word_embeddings = model.get_text_features(**inputs)\n",
    "    \n",
    "    # Normalize embeddings for cosine similarity\n",
    "    word_embeddings /= word_embeddings.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Convert to numpy for scikit-learn\n",
    "    embeddings_np = word_embeddings.cpu().numpy()\n",
    "\n",
    "    # 2. Perform clustering to find 4 groups of 4\n",
    "    # AgglomerativeClustering is great for this. It builds clusters by successively\n",
    "    # merging the closest data points.\n",
    "    # - n_clusters=4: We know we want exactly four groups.\n",
    "    # - affinity='cosine': The distance metric to use. Cosine similarity is perfect for embeddings.\n",
    "    # - linkage='average': How to calculate the distance between clusters. 'average' is a robust choice.\n",
    "    print(\"Clustering embeddings...\")\n",
    "    clustering = AgglomerativeClustering(\n",
    "        n_clusters=4,\n",
    "        metric='cosine', # Note: scikit-learn uses cosine distance (1-sim), which is equivalent for clustering\n",
    "        linkage='average'\n",
    "    ).fit(embeddings_np)\n",
    "\n",
    "    # 3. Group the words based on cluster labels\n",
    "    groups = defaultdict(list)\n",
    "    for i, word in enumerate(words):\n",
    "        groups[clustering.labels_[i]].append(word)\n",
    "\n",
    "    # 4. (Bonus) Find the theme for each group\n",
    "    # We do this by calculating the average embedding for each cluster (its \"centroid\")\n",
    "    # and then finding which single word in the vocabulary is closest to that centroid.\n",
    "    print(\"Finding group themes...\")\n",
    "    group_themes = {}\n",
    "    \n",
    "    # Get the model's vocabulary to search for theme words\n",
    "    vocab = list(tokenizer.get_vocab().keys())\n",
    "    # Clean up tokens for better readability\n",
    "    clean_vocab = [word.replace(\"</w>\", \"\").lower() for word in vocab if '</w>' in word and len(word) > 3]\n",
    "    clean_vocab = list(set(clean_vocab)) # Remove duplicates\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Pre-encode the entire vocabulary for faster search. This might take a moment.\n",
    "        # For very large vocabs, you might sample, but CLIP's is manageable.\n",
    "        print(f\"Encoding {len(clean_vocab)} vocabulary words for theme search...\")\n",
    "        vocab_inputs = tokenizer(clean_vocab, padding=True, return_tensors=\"pt\").to(device)\n",
    "        vocab_embeddings = model.get_text_features(**vocab_inputs)\n",
    "        vocab_embeddings /= vocab_embeddings.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    for group_id, member_words in groups.items():\n",
    "        # Find the embeddings for the words in the current group\n",
    "        member_indices = [words.index(w) for w in member_words]\n",
    "        member_embeddings = torch.tensor(embeddings_np[member_indices]).to(device)\n",
    "        \n",
    "        # Calculate the average embedding (centroid) for the group\n",
    "        centroid = member_embeddings.mean(dim=0, keepdim=True)\n",
    "        \n",
    "        # Find the most similar word in the vocabulary to this centroid\n",
    "        similarities = torch.nn.functional.cosine_similarity(centroid, vocab_embeddings)\n",
    "        best_match_index = similarities.argmax()\n",
    "        theme_guess = clean_vocab[best_match_index]\n",
    "        \n",
    "        group_themes[group_id] = theme_guess.upper()\n",
    "\n",
    "    return dict(groups), dict(group_themes)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- Setup ---\n",
    "    device = \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load the model and tokenizer once\n",
    "    print(\"Loading CLIP model and tokenizer...\")\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    # --- Puzzle Definition ---\n",
    "    # Example puzzle from a past NYT Connections game (April 5, 2024)\n",
    "    puzzle_words = [\n",
    "        \"DOG\", \"LITTLE SPOON\", \"IRIS\", \"WATER BOTTLE\",\n",
    "        \"TOO LOSE\", \"LENS\", \"BENT\", \"ROD\",\n",
    "        \"CONE\", \"ONE WEEK\", \"POTATO\", \"POINT OF VIEW\",\n",
    "        \"ANGLE\", \"CUP\", \"CLOSING TIME\", \"SCOOP\"\n",
    "    ]\n",
    "\n",
    "    # --- Solve the Puzzle ---\n",
    "    try:\n",
    "        solved_groups, themes = solve_connections(puzzle_words, model, tokenizer, device)\n",
    "\n",
    "        # --- Display Results ---\n",
    "        print(\"\\n--- Connections Puzzle Solved! ---\\n\")\n",
    "        for group_id, words_in_group in solved_groups.items():\n",
    "            theme = themes.get(group_id, \"UNKNOWN\")\n",
    "            print(f\"ðŸ”µ Group (Predicted Theme: {theme}): {', '.join(words_in_group)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
