{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f99b35f1-31ec-445a-b6b9-7d9086ac2431",
   "metadata": {},
   "source": [
    "# Lab 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20362be4-cbfc-4075-88c5-43f510b59c4a",
   "metadata": {},
   "source": [
    "This week we look at a new dataset below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fc2d46-9c8d-4519-a0b7-a0b70ef9ac2f",
   "metadata": {},
   "source": [
    "!curl https://storage.googleapis.com/aiolympiadmy/ioai-2025-tsp/lab3.zip -o lab3.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa019876-fdf7-49b7-973a-95fdc96819a8",
   "metadata": {},
   "source": [
    "!unzip -q lab3.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bf9ede-36cb-4a96-890b-4faedcd5be44",
   "metadata": {},
   "source": [
    "The above commands should create a `train/` folder and `test/` folder in this directory. Each folder contains `images/` and `labels/` subdirectories respectively.\n",
    "\n",
    "In this dataset, each image is associated with a text file. In the text file, each row represents a single bounding box around a person in the image. Each bounding box is represented by 5 numbers: x_center, y_center, width, height, objectness. Objectness is always 1. All other dimensions are represented as fractions of image dimensions. e.g. x_center's actual pixel location needs to be multiplied by the width of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843c3bce-23f5-4e14-9a45-0c1f768e3a9b",
   "metadata": {},
   "source": [
    "If you still recall the network you worked with in Lab 2:\n",
    "```python\n",
    "class FCN(nn.Module):\n",
    "    def __init__(self, backbone, head):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = head\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "backbone = ...\n",
    "head = ...\n",
    "model = FCN(backbone, head)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5505b59-c0ea-4da4-9157-dd87f35a5611",
   "metadata": {},
   "source": [
    "Here's an idea. Given that the backbone defined above will output a spatial feature map, we can iterate over each cell location in the spatial feature map, and predict if there is an object to detect in that cell (objectness). Notice that this is very similar to FCN segmentation where we were practically doing pixel-wise classification. However, we take this a bit further. If objectness in each cell is high enough, we predict an object within a bounding box as specified by our neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682e4b96-f456-4965-9707-6a184ea28867",
   "metadata": {},
   "source": [
    "## Another new network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147dc66e-1bd2-4d2b-bf19-9f16d735edb6",
   "metadata": {},
   "source": [
    "Create a new class that is almost the exact same as `FCN` above, but specify the `head` to be a `nn.Conv2d` layer with kernel size 1. This layer should output 5 channels, corresponding to the 5 numbers in each row of our labels. This network will be architecturally simpler than the FCN of Lab 2!\n",
    "\n",
    "_1 pt granted upon completion of network definition_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b7803d7-46c0-4866-b508-26717790e6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ochon\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ochon\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "class FCN(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = nn.Conv2d(in_channels=512, out_channels=5, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "device = 'xpu'\n",
    "\n",
    "backbone = models.resnet34(pretrained=True)\n",
    "backbone = nn.Sequential(*list(backbone.children())[:-2])\n",
    "backbone.add_module('adaptive_pool', nn.AdaptiveAvgPool2d((8, 8)))\n",
    "\n",
    "model = FCN(backbone).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d73930-e9d7-4958-8369-bf59f446fc21",
   "metadata": {},
   "source": [
    "## Dataset and dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9a10cb-7a06-4964-9e20-1a6a6bd2e334",
   "metadata": {},
   "source": [
    "Now go ahead and build your Dataset and Dataloaders in Pytorch.\n",
    "\n",
    "Note that the dataset should calculate and return x_offset and y_offset instead of x_center and y_center. If you leave x_center and y_center as is, you will force your network to also learn how to predict larger values of x_center and y_center with increasing values of x and y on the spatial feature map output by the backbone! You can resolve this by storing the offset between the bounding box center and the center of the cell of the spatial feature map your bounding box is in. I'll help you a little by providing an example:\n",
    "\n",
    "```python\n",
    "ipdb>  grid_size\n",
    "8\n",
    "\n",
    "ipdb>  grid_y\n",
    "4\n",
    "\n",
    "ipdb>  grid_y_min\n",
    "0.5\n",
    "\n",
    "ipdb>  y_center\n",
    "0.5132275132275133\n",
    "\n",
    "ipdb>  y_offset\n",
    "0.013227513227513255\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd74f81-d191-46a7-8e44-977607bcb7bd",
   "metadata": {},
   "source": [
    "_1 pt granted upon successfully running the code below_,\n",
    "\n",
    "```python\n",
    "train_dataset = ...\n",
    "test_dataset = ...\n",
    "train_dataloader = DataLoader(train_dataset, ...)\n",
    "test_dataloader = DataLoader(test_dataset, ...)\n",
    "one_X, one_y = next(iter(test_dataset))\n",
    "batch_X, batch_y = next(iter(test_dataloader))\n",
    "```\n",
    "\n",
    "_and demonstrating the following results_:\n",
    "- `one_X.shape` = (3, im_h, im_w) where im_h and im_w are height and width of the image\n",
    "- `one_y.shape` = (5, gy, gx) where gy and gx are height and width of the spatial feature map\n",
    "- `batch_X.shape` = (B, 3, im_h, im_w) where B is batch size\n",
    "- `batch_y.shape` = (B, 5, gy, gx)\n",
    "\n",
    "Unless you've went out of your way to resize your images to a shape other than a square, `im_h` should be equal to `im_w`, and `gy` should be equal to `gx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64142e9b-716f-4b79-86b9-bab604135387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample loaded successfully:\n",
      "Image shape: torch.Size([3, 416, 416])\n",
      "Target shape: torch.Size([5, 8, 8])\n",
      "Train images: 136\n",
      "Test images: 34\n",
      "one_X.shape: torch.Size([3, 416, 416])\n",
      "one_y.shape: torch.Size([5, 8, 8])\n",
      "batch_X.shape: torch.Size([4, 3, 416, 416])\n",
      "batch_y.shape: torch.Size([4, 5, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class DetectionDataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.images_dir = os.path.join(root_dir, split, 'images')\n",
    "        self.labels_dir = os.path.join(root_dir, split, 'labels')\n",
    "        \n",
    "        # Get image filenames (PNGs)\n",
    "        self.image_files = [f for f in os.listdir(self.images_dir) if f.endswith('.png')]\n",
    "        self.label_files = [f.replace('.png', '.txt') for f in self.image_files]\n",
    "        \n",
    "        # Grid dimensions (gy and gx are both 8)\n",
    "        self.gy = 8\n",
    "        self.gx = 8\n",
    "\n",
    "        # Default transform: Resize to 416x416 and convert to tensor\n",
    "        if self.transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((416, 416)),  # Resize to fixed dimensions\n",
    "                transforms.ColorJitter(0.2, 0.2, 0.2),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.ToTensor()          # Converts to [0,1] and (C, H, W)\n",
    "            ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.images_dir, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image_tensor = self.transform(image)  # Apply transform to PIL Image\n",
    "        \n",
    "        # Load labels and compute targets\n",
    "        targets = torch.zeros(5, self.gy, self.gx)  # [x_offset, y_offset, width, height, objectness]\n",
    "        label_path = os.path.join(self.labels_dir, self.label_files[idx])\n",
    "        \n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f:\n",
    "                data = line.strip().split()\n",
    "                x_center, y_center, width, height, _ = map(float, data)  # objectness is always 1\n",
    "                \n",
    "                # Find cell indices (i, j)\n",
    "                j = int(x_center * self.gx)  # Column index\n",
    "                i = int(y_center * self.gy)  # Row index\n",
    "                \n",
    "                # Compute cell's center (normalized coordinates)\n",
    "                cell_x = (j + 0.5) / self.gx\n",
    "                cell_y = (i + 0.5) / self.gy\n",
    "                \n",
    "                # Compute offsets\n",
    "                x_offset = x_center - cell_x\n",
    "                y_offset = y_center - cell_y\n",
    "                \n",
    "                # Update targets for this cell\n",
    "                targets[0, i, j] = x_offset\n",
    "                targets[1, i, j] = y_offset\n",
    "                targets[2, i, j] = width\n",
    "                targets[3, i, j] = height\n",
    "                targets[4, i, j] = 1.0  # Objectness (always 1 for valid boxes)\n",
    "\n",
    "        return image_tensor, targets\n",
    "\n",
    "# Example usage:\n",
    "train_dataset = DetectionDataset(root_dir='lab3', split='train')\n",
    "test_dataset = DetectionDataset(root_dir='lab3', split='test')\n",
    "\n",
    "# Check first sample\n",
    "try:\n",
    "    img, target = train_dataset[0]\n",
    "    print(\"Sample loaded successfully:\")\n",
    "    print(\"Image shape:\", img.shape)\n",
    "    print(\"Target shape:\", target.shape)\n",
    "except Exception as e:\n",
    "    print(\"Error loading dataset:\", e)\n",
    "\n",
    "# Create Dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "print(f\"Train images: {len(train_dataset.image_files)}\")  # Should be 2 (A00002.png, A00003.png)\n",
    "print(f\"Test images: {len(test_dataset.image_files)}\")   # Should be 2 (A00001.png, A00005.png)\n",
    "\n",
    "# Verify shapes (replace with actual code)\n",
    "one_X, one_y = next(iter(test_dataset))\n",
    "print(\"one_X.shape:\", one_X.shape)  # Should be (3, H, W)\n",
    "print(\"one_y.shape:\", one_y.shape)  # Should be (5, 8, 8)\n",
    "\n",
    "batch_X, batch_y = next(iter(test_dataloader))\n",
    "print(\"batch_X.shape:\", batch_X.shape)  # (4, 3, H, W)\n",
    "print(\"batch_y.shape:\", batch_y.shape)  # (4, 5, 8, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789fae47-d37f-4de1-9bbf-b0427103af8b",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbe2a2b-052d-4f10-b0ca-831150aead9f",
   "metadata": {},
   "source": [
    "Being able to see is important for diagnosing computer vision applications. Create a visualization function to plot an overlay of bounding boxes on their respective images. You can use this function template below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59da2c72-d35a-41de-b13b-6f9d30793fa2",
   "metadata": {},
   "source": [
    "```python\n",
    "def plot_batch_predictions(imgs, outputs):\n",
    "    # img should have shape (batch, 3, im_h, im_w)\n",
    "    # outputs should have shape (batch, 5, gy, gx)\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11021cb-c38e-44ed-95e9-2bab37f68df5",
   "metadata": {},
   "source": [
    "Will also show you some sample matplotlib code to save time:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This creates a matplotlib figure with 4 cols\n",
    "# Note that axes is an array of individual `ax`\n",
    "batch_size = 4\n",
    "fig, axes = plt.subplots(ncols=batch_size, figsize=(20, 8))\n",
    "axes = axes if isinstance(axes, np.ndarray) else [axes]  # Handle batch_size=1\n",
    "\n",
    "# This is how to plot an image\n",
    "# Note that imshow requires image dimensions to be (H, W, 3)\n",
    "# while Pytorch works with image dimensions (3, H, W)!\n",
    "mock_tensor = torch.rand(3, 128, 128)\n",
    "mock_np = mock_tensor.permute(1, 2, 0).contiguous().numpy()\n",
    "# Usually you would just use plt.imshow where plt will grab the latest ax\n",
    "# When you have as many as 4 in this example, specify which ax to use\n",
    "ax = axes[0]\n",
    "ax.imshow(img_np)\n",
    "    \n",
    "# This is how to draw rectangles using matplotlib\n",
    "xmin = int((x_center - width / 2) * im_width)\n",
    "xmax = int((x_center + width / 2) * im_width)\n",
    "rect = plt.Rectangle(\n",
    "    (xmin, ymin), width, height, \n",
    "    linewidth=2, edgecolor='red', facecolor='none'\n",
    ")\n",
    "# Add the rectangle to the Axes\n",
    "ax.add_patch(rect)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53090302-7f08-4a08-b4e5-bfe43f4a313d",
   "metadata": {},
   "source": [
    "Remember that your network output is x_offset and y_offset, need to convert them back to x_center and y_center!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4500fd67-7450-4186-8686-3f62da7be390",
   "metadata": {},
   "source": [
    "_1 pt granted upon plotting one batch of images and labels from `test_dataloader` using `plot_batch_predictions`_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5c0c77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def plot_batch_predictions(imgs, outputs, threshold=0.5):\n",
    "    batch_size = imgs.shape[0]\n",
    "    fig, axes = plt.subplots(ncols=batch_size, figsize=(20, 8))\n",
    "    \n",
    "    if batch_size == 1:  # Handle single-image batches\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx in range(batch_size):\n",
    "        ax = axes[idx]\n",
    "        # Convert tensor to numpy image (H, W, 3)\n",
    "        img = imgs[idx].cpu().permute(1, 2, 0).numpy()\n",
    "        ax.imshow(img)\n",
    "        \n",
    "        output = outputs[idx].cpu().detach()\n",
    "        gy, gx = output.shape[1], output.shape[2]  # Grid dimensions (8x8)\n",
    "        \n",
    "        # Convert outputs to bounding boxes\n",
    "        for i in range(gy):\n",
    "            for j in range(gx):\n",
    "                obj_prob = torch.sigmoid(output[4, i, j])\n",
    "                if obj_prob > threshold:\n",
    "                    # Get predicted box parameters\n",
    "                    x_offset = output[0, i, j].item()\n",
    "                    y_offset = output[1, i, j].item()\n",
    "                    width = output[2, i, j].item()\n",
    "                    height = output[3, i, j].item()\n",
    "                    \n",
    "                    # Calculate absolute coordinates\n",
    "                    cell_x = (j + 0.5) / gx  # Normalized cell center\n",
    "                    cell_y = (i + 0.5) / gy\n",
    "                    x_center = (cell_x + x_offset) * img.shape[1]  # Pixel coordinates\n",
    "                    y_center = (cell_y + y_offset) * img.shape[0]\n",
    "                    w = width * img.shape[1]\n",
    "                    h = height * img.shape[0]\n",
    "                    \n",
    "                    # Create rectangle\n",
    "                    rect = plt.Rectangle(\n",
    "                        (x_center - w/2, y_center - h/2), w, h,\n",
    "                        linewidth=2, edgecolor='red', facecolor='none'\n",
    "                    )\n",
    "                    ax.add_patch(rect)\n",
    "        \n",
    "        ax.axis('off')  # Remove axis ticks\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7f924e",
   "metadata": {},
   "source": [
    "# Get a batch from dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afe0a6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_X, _ = next(iter(test_dataloader))\n",
    "batch_X = batch_X.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b8290f",
   "metadata": {},
   "source": [
    "# Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d25bd250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  outputs = model(batch_X)\n",
    "\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb954f12",
   "metadata": {},
   "source": [
    "# Visualize predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae58b50",
   "metadata": {},
   "source": [
    "# NOTE: ONLY RUN THIS CELL BELOW IN GOOGLE COLAB / KAGGLE (not sure) else your Kernel will crash (at least mine crashed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dc700d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#plot_batch_predictions(batch_X, outputs, threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628bea59-cb42-4d66-9544-2f2819fdb61a",
   "metadata": {},
   "source": [
    "## Setting up loss calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fd4c51-208b-4865-a223-f2bf1b24fff7",
   "metadata": {},
   "source": [
    "Objectness is essentially a binary classification task, while predicting the correct bounding box is a regression task. \n",
    "\n",
    "Set up loss function calculations for objectness using binary cross entropy, and for bounding box localization using MSE. Create both losses in the same function for convenience, but return them separately instead of as a sum so they are easy to log later on. \n",
    "\n",
    "Concept-wise this is pretty straightforward. However, implementation-wise, you will need to place your tensors with great care. I'll help you a bit by providing you with this template below.\n",
    "\n",
    "```python\n",
    "def custom_loss(preds, targets):\n",
    "    # both preds and targets should have shape (B, 5, gy, gx)\n",
    "    # where B is batch size, gy and gx are spatial feature map h and w \n",
    "    ...\n",
    "    return objectness_loss, localization_loss\n",
    "```\n",
    "\n",
    "_1 pt granted upon completion of loss function calculation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a72672ab-6d7c-4f53-abf4-1ab1ee5058c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def custom_loss(preds, targets):\n",
    "    \"\"\"Calculate objectness (BCE) and localization (MSE) losses\"\"\"\n",
    "    # Objectness loss (binary classification)\n",
    "    obj_preds = preds[:, 4, :, :]  # (B, gy, gx)\n",
    "    obj_targets = targets[:, 4, :, :]\n",
    "    obj_loss = F.binary_cross_entropy_with_logits(obj_preds, obj_targets)\n",
    "    \n",
    "    # Localization loss (regression)\n",
    "    loc_preds = preds[:, :4, :, :]  # (B, 4, gy, gx)\n",
    "    loc_targets = targets[:, :4, :, :]\n",
    "    \n",
    "    # Mask for cells containing objects\n",
    "    mask = (obj_targets > 0.5).unsqueeze(1)  # (B, 1, gy, gx)\n",
    "    mask = mask.expand_as(loc_preds)  # Match dimensions (B, 4, gy, gx)\n",
    "    \n",
    "    # Filter predictions and targets using mask\n",
    "    loc_preds_filtered = loc_preds[mask]\n",
    "    loc_targets_filtered = loc_targets[mask]\n",
    "    \n",
    "    # Calculate MSE loss only for positive cells\n",
    "    loc_loss = F.mse_loss(loc_preds_filtered, loc_targets_filtered) if loc_preds_filtered.numel() > 0 \\\n",
    "               else torch.tensor(0.0, device=preds.device)\n",
    "    \n",
    "    return obj_loss, loc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea4352a-b1ff-45c6-8ff7-6a02a78ac16a",
   "metadata": {},
   "source": [
    "## Model evalution and baseline score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55001e3-7270-428c-b337-cb13aa6b42e2",
   "metadata": {},
   "source": [
    "Create a `test_one_epoch` function that takes the model and the test dataloader as arguments. Calculate and return box IOU score (`torchvision.ops.box_iou`) in a dictionary like so:\n",
    "\n",
    "```python\n",
    ">>> metrics = test_one_epoch(model, test_dataloader)\n",
    ">>> print(metrics)\n",
    "{\"miou\": 0.005}\n",
    "```\n",
    "\n",
    "_1 pt granted upon implementing `test_one_epoch` and seeing the mean IOU score of the untrained model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de1ec4dd-9fcb-4589-b082-bea8454d5ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import box_iou\n",
    "\n",
    "def test_one_epoch(model, test_loader, threshold=0.5, img_size=416):\n",
    "    model.eval()\n",
    "    total_iou = 0.0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            \n",
    "            # Convert outputs and targets to boxes\n",
    "            for i in range(batch_X.size(0)):  # Iterate through batch\n",
    "                # Get predictions for this image\n",
    "                pred_boxes = []\n",
    "                output = outputs[i]  # (5, gy, gx)\n",
    "                gy, gx = output.shape[1], output.shape[2]\n",
    "                \n",
    "                # Convert model outputs to boxes\n",
    "                for y in range(gy):\n",
    "                    for x in range(gx):\n",
    "                        obj_score = torch.sigmoid(output[4, y, x])\n",
    "                        if obj_score > threshold:\n",
    "                            # Calculate box coordinates\n",
    "                            x_offset = output[0, y, x].item()\n",
    "                            y_offset = output[1, y, x].item()\n",
    "                            width = output[2, y, x].item()\n",
    "                            height = output[3, y, x].item()\n",
    "                            \n",
    "                            # Convert to pixel coordinates\n",
    "                            cell_x = (x + 0.5) / gx\n",
    "                            cell_y = (y + 0.5) / gy\n",
    "                            x_center = (cell_x + x_offset) * img_size\n",
    "                            y_center = (cell_y + y_offset) * img_size\n",
    "                            w = width * img_size\n",
    "                            h = height * img_size\n",
    "                            \n",
    "                            pred_boxes.append(torch.tensor([\n",
    "                                x_center - w/2,  # xmin\n",
    "                                y_center - h/2,  # ymin\n",
    "                                x_center + w/2,  # xmax\n",
    "                                y_center + h/2   # ymax\n",
    "                            ]))\n",
    "                \n",
    "                # Convert ground truth to boxes\n",
    "                gt_boxes = []\n",
    "                target = batch_y[i]  # (5, gy, gx)\n",
    "                for y in range(gy):\n",
    "                    for x in range(gx):\n",
    "                        if target[4, y, x] == 1:  # Object exists\n",
    "                            x_offset = target[0, y, x].item()\n",
    "                            y_offset = target[1, y, x].item()\n",
    "                            width = target[2, y, x].item()\n",
    "                            height = target[3, y, x].item()\n",
    "                            \n",
    "                            # Convert to pixel coordinates\n",
    "                            cell_x = (x + 0.5) / gx\n",
    "                            cell_y = (y + 0.5) / gy\n",
    "                            x_center = (cell_x + x_offset) * img_size\n",
    "                            y_center = (cell_y + y_offset) * img_size\n",
    "                            w = width * img_size\n",
    "                            h = height * img_size\n",
    "                            \n",
    "                            gt_boxes.append(torch.tensor([\n",
    "                                x_center - w/2,\n",
    "                                y_center - h/2,\n",
    "                                x_center + w/2,\n",
    "                                y_center + h/2\n",
    "                            ]))\n",
    "                \n",
    "                # Calculate IoU if we have both predictions and ground truth\n",
    "                if pred_boxes and gt_boxes:\n",
    "                    pred_tensor = torch.stack(pred_boxes).to(device)\n",
    "                    gt_tensor = torch.stack(gt_boxes).to(device)\n",
    "                    \n",
    "                    iou_matrix = box_iou(pred_tensor, gt_tensor)\n",
    "                    best_ious = iou_matrix.max(dim=0).values  # For each GT box\n",
    "                    mean_iou = best_ious.mean().item()\n",
    "                    \n",
    "                    total_iou += mean_iou\n",
    "                    total_samples += 1\n",
    "                elif gt_boxes:  # No predictions but has GT (count as 0 IoU)\n",
    "                    total_samples += 1\n",
    "\n",
    "    return {\"miou\": total_iou / total_samples if total_samples else 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0f6e32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained model mIoU: 0.0007\n"
     ]
    }
   ],
   "source": [
    "metrics = test_one_epoch(model, test_dataloader)\n",
    "print(f\"Untrained model mIoU: {metrics['miou']:.4f}\")\n",
    "# Typical output: {\"miou\": 0.001-0.01} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7438a4c9-4775-4d3d-88d8-4b01ef23b747",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdfa113-16ef-4705-8e20-fe359abb6193",
   "metadata": {},
   "source": [
    "Train your model on the training set. Track objectness loss and localization loss during training for every 10 minibatches (a). I will leave it up to choose how to combine your losses. \n",
    "\n",
    "At the end of every epoch, show metrics on both train (b) and test data (c), and plot prediction outputs of the first batch of the test dataset (d). Save the best performing model with the highest mean IOU score on test (e).\n",
    "\n",
    "You don't need to run training for too long. I suspect <50 epochs will be sufficient.\n",
    "\n",
    "_1 pt granted upon completion of (a) to (e)._\n",
    "\n",
    "_Another 1 pt granted for exceeding 0.4 mean IOU on the test dataset._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c10b229c-280a-4e1c-af54-49172ea4aa27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ochon\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 10: Obj Loss: 0.2878, Loc Loss: 0.8711\n",
      "Epoch 1 Batch 20: Obj Loss: 0.2183, Loc Loss: 0.4515\n",
      "Epoch 1 Batch 30: Obj Loss: 0.1941, Loc Loss: 0.3552\n",
      "\n",
      "Epoch 1 Results:\n",
      "Train mIoU: 0.0000\n",
      "Test mIoU: 0.0000\n",
      "Objectness Loss: 0.1883\n",
      "Localization Loss: 0.3192\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Training setup\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.2, verbose=True)\n",
    "best_miou = 0.0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    epoch_obj_loss = 0.0\n",
    "    epoch_loc_loss = 0.0\n",
    "    \n",
    "    # Training phase\n",
    "    for batch_idx, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        obj_loss, loc_loss = custom_loss(outputs, y)\n",
    "        total_loss = obj_loss + loc_loss\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_obj_loss += obj_loss.item()\n",
    "        epoch_loc_loss += loc_loss.item()\n",
    "\n",
    "        # Log every 10 batches\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            avg_obj = epoch_obj_loss / (batch_idx + 1)\n",
    "            avg_loc = epoch_loc_loss / (batch_idx + 1)\n",
    "            print(f\"Epoch {epoch+1} Batch {batch_idx+1}: \"\n",
    "                  f\"Obj Loss: {avg_obj:.4f}, Loc Loss: {avg_loc:.4f}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    train_metrics = test_one_epoch(model, train_dataloader)\n",
    "    test_metrics = test_one_epoch(model, test_dataloader)\n",
    "    scheduler.step(test_metrics['miou'])\n",
    "\n",
    "    print(f\"\\nEpoch {epoch+1} Results:\")\n",
    "    print(f\"Train mIoU: {train_metrics['miou']:.4f}\")\n",
    "    print(f\"Test mIoU: {test_metrics['miou']:.4f}\")\n",
    "    print(f\"Objectness Loss: {epoch_obj_loss/len(train_dataloader):.4f}\")\n",
    "    print(f\"Localization Loss: {epoch_loc_loss/len(train_dataloader):.4f}\\n\")\n",
    "\n",
    "    # Save best model\n",
    "    if test_metrics['miou'] > best_miou:\n",
    "        best_miou = test_metrics['miou']\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        print(f\"New best model saved with mIoU: {best_miou:.4f}\")\n",
    "\n",
    "    # Visualization\n",
    "    test_sample = next(iter(test_dataloader))\n",
    "    test_images, _ = test_sample\n",
    "    with torch.no_grad():\n",
    "        outputs = model(test_images.to(device))\n",
    "    plot_batch_predictions(test_images, outputs.cpu())\n",
    "\n",
    "print(f\"Training complete. Best Test mIoU: {best_miou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcfe98f-bb40-45c1-95e2-951c055b3c94",
   "metadata": {},
   "source": [
    "## Post training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a683d67-61b3-4881-88c4-14a36c3a7908",
   "metadata": {},
   "source": [
    "Create a plot that contains four subplots: image with true bounding boxes, image with predicted bounding boxes, predicted objectness over spatial feature map, true objectness over spatial feature map). Repeat this plot for a few images.\n",
    "\n",
    "_1 pt granted upon completion of the above_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a74907-a0c3-416c-ad5f-2c6ea10c63b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(imgs, outputs, targets):\n",
    "    # Plot images with predicted and true boxes, objectness heatmaps..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4af3f31-97d8-4bc9-b16f-e3202856c4eb",
   "metadata": {},
   "source": [
    "What learning task did you just perform in this notebook?\n",
    "\n",
    "_1 pt granted upon finding the right answer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a223ee49-3425-445f-951a-1921385c53f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc8a433-58a1-4816-9ab2-d7311fdbbd85",
   "metadata": {},
   "source": [
    "This dataset is 17 years old this year. What is it called?\n",
    "\n",
    "_1 pt granted upon finding the right answer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd521e87-10d1-4787-a053-440d2b084af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88635fcd-dc3c-4cbb-8ae0-6108911c8df2",
   "metadata": {},
   "source": [
    "## EX: Going off track"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec61952-3018-42dc-ac7d-4f6fdf2c3147",
   "metadata": {},
   "source": [
    "Name a limitation of this training setup and briefly explain your reasoning\n",
    "\n",
    "_1 pt granted upon a satisfactory answer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c611d0b7-842d-402b-91ef-2ddd084c6c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your work here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a984d5-4a19-4ccb-87d5-fd717e70cc88",
   "metadata": {},
   "source": [
    "Show me how you can modify this training setup to attain better performance.\n",
    "\n",
    "_2 pts granted upon successfully scoring at least +0.2 mean IOU higher than the score of the best model above. Partial credit to be granted at discretion. Bonus additional +1 pt to be granted for outstanding improvements_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc61777-860a-4604-8d58-c16883212be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your work here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
